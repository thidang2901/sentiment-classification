{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "test-nlp.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DzR5fcj4EqB5"
      },
      "source": [
        "# Prepare things"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uklR4wugkDdY"
      },
      "source": [
        "## Install requirements"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KoAiOX9JShI8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "04005ef2-098c-43a0-9618-e6b1e828bdb8"
      },
      "source": [
        "!pip install underthesea\n",
        "!pip install emoji"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting underthesea\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a8/5f/03ab9091b88e7851aa92da33f8eea6f111423cc1194cf1636c63c1fff3d0/underthesea-1.3.1-py3-none-any.whl (7.5MB)\n",
            "\u001b[K     |████████████████████████████████| 7.5MB 7.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: Click>=6.0 in /usr/local/lib/python3.7/dist-packages (from underthesea) (7.1.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from underthesea) (4.41.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from underthesea) (3.2.5)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from underthesea) (3.13)\n",
            "Collecting transformers<=3.5.1,>=3.5.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3a/83/e74092e7f24a08d751aa59b37a9fc572b2e4af3918cb66f7766c3affb1b4/transformers-3.5.1-py3-none-any.whl (1.3MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3MB 54.0MB/s \n",
            "\u001b[?25hCollecting python-crfsuite>=0.9.6\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/79/47/58f16c46506139f17de4630dbcfb877ce41a6355a1bbf3c443edb9708429/python_crfsuite-0.9.7-cp37-cp37m-manylinux1_x86_64.whl (743kB)\n",
            "\u001b[K     |████████████████████████████████| 747kB 53.2MB/s \n",
            "\u001b[?25hCollecting unidecode\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9e/25/723487ca2a52ebcee88a34d7d1f5a4b80b793f179ee0f62d5371938dfa01/Unidecode-1.2.0-py2.py3-none-any.whl (241kB)\n",
            "\u001b[K     |████████████████████████████████| 245kB 51.1MB/s \n",
            "\u001b[?25hCollecting seqeval\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9d/2d/233c79d5b4e5ab1dbf111242299153f3caddddbb691219f363ad55ce783d/seqeval-1.2.2.tar.gz (43kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 8.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from underthesea) (0.22.2.post1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from underthesea) (1.0.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from underthesea) (2.23.0)\n",
            "Collecting torch<=1.5.1,>=1.1.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a4/cf/007b6de316c9f3d4cb315a60c308342cc299e464167f5ebc369e93b5e23a/torch-1.5.1-cp37-cp37m-manylinux1_x86_64.whl (753.2MB)\n",
            "\u001b[K     |████████████████████████████████| 753.2MB 19kB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk->underthesea) (1.15.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers<=3.5.1,>=3.5.0->underthesea) (3.0.12)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers<=3.5.1,>=3.5.0->underthesea) (20.9)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/75/ee/67241dc87f266093c533a2d4d3d69438e57d7a90abb216fa076e7d475d4a/sacremoses-0.0.45-py3-none-any.whl (895kB)\n",
            "\u001b[K     |████████████████████████████████| 901kB 55.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from transformers<=3.5.1,>=3.5.0->underthesea) (1.19.5)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers<=3.5.1,>=3.5.0->underthesea) (2019.12.20)\n",
            "Collecting sentencepiece==0.1.91\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f2/e2/813dff3d72df2f49554204e7e5f73a3dc0f0eb1e3958a4cad3ef3fb278b7/sentencepiece-0.1.91-cp37-cp37m-manylinux1_x86_64.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 46.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: protobuf in /usr/local/lib/python3.7/dist-packages (from transformers<=3.5.1,>=3.5.0->underthesea) (3.12.4)\n",
            "Collecting tokenizers==0.9.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7b/ac/f5ba028f0f097d855e1541301e946d4672eb0f30b6e25cb2369075f916d2/tokenizers-0.9.3-cp37-cp37m-manylinux1_x86_64.whl (2.9MB)\n",
            "\u001b[K     |████████████████████████████████| 2.9MB 39.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->underthesea) (1.4.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->underthesea) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->underthesea) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->underthesea) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->underthesea) (2.10)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from torch<=1.5.1,>=1.1.0->underthesea) (0.16.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers<=3.5.1,>=3.5.0->underthesea) (2.4.7)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf->transformers<=3.5.1,>=3.5.0->underthesea) (56.0.0)\n",
            "Building wheels for collected packages: seqeval\n",
            "  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for seqeval: filename=seqeval-1.2.2-cp37-none-any.whl size=16172 sha256=5d13fb3cef4415c44749a4ec7dc0e7845a60982fa49af894a1e1b9b72dfb538e\n",
            "  Stored in directory: /root/.cache/pip/wheels/52/df/1b/45d75646c37428f7e626214704a0e35bd3cfc32eda37e59e5f\n",
            "Successfully built seqeval\n",
            "\u001b[31mERROR: torchvision 0.9.1+cu101 has requirement torch==1.8.1, but you'll have torch 1.5.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: torchtext 0.9.1 has requirement torch==1.8.1, but you'll have torch 1.5.1 which is incompatible.\u001b[0m\n",
            "Installing collected packages: sacremoses, sentencepiece, tokenizers, transformers, python-crfsuite, unidecode, seqeval, torch, underthesea\n",
            "  Found existing installation: torch 1.8.1+cu101\n",
            "    Uninstalling torch-1.8.1+cu101:\n",
            "      Successfully uninstalled torch-1.8.1+cu101\n",
            "Successfully installed python-crfsuite-0.9.7 sacremoses-0.0.45 sentencepiece-0.1.91 seqeval-1.2.2 tokenizers-0.9.3 torch-1.5.1 transformers-3.5.1 underthesea-1.3.1 unidecode-1.2.0\n",
            "Collecting emoji\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/24/fa/b3368f41b95a286f8d300e323449ab4e86b85334c2e0b477e94422b8ed0f/emoji-1.2.0-py3-none-any.whl (131kB)\n",
            "\u001b[K     |████████████████████████████████| 133kB 6.9MB/s \n",
            "\u001b[?25hInstalling collected packages: emoji\n",
            "Successfully installed emoji-1.2.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zFkWiOqGkKO9"
      },
      "source": [
        "## Mount to google drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "358078LAaYfB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "87c1f863-cb1f-4f99-feda-48b145f94250"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "!ls '/content/gdrive/My Drive/Colab Notebooks/dataset/'"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n",
            "train_dataset.xlsx\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kLvxqX45kYik"
      },
      "source": [
        "# IMPORTANT! Read file into dataframe\n",
        "\n",
        "*   Check the Google Drive path which contains the dataset file.\n",
        "*   Input the correct file name, also the column names which are used for comment and sentiment.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VT462fSmFXWS"
      },
      "source": [
        "path = \"/content/gdrive/My Drive/Colab Notebooks\"\n",
        "\n",
        "FILE_NAME = \"train_dataset.xlsx\"\n",
        "COMMENT_COLUMN_NAME = \"comment\"\n",
        "SENTIMENT_COLUMN_NAME = \"sentiment\""
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "4WVeHsWbeMu6",
        "outputId": "23b4a3d5-3291-46dc-ec4e-0e30ee802721"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_excel(f'{path}/dataset/{FILE_NAME}', usecols=[\"id\", COMMENT_COLUMN_NAME, SENTIMENT_COLUMN_NAME])\n",
        "df.head()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>comment</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2</td>\n",
              "      <td>Bài của Phan Mạnh Quỳnh qá hay xuất sắc, hẹn b...</td>\n",
              "      <td>Tích cực</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>3</td>\n",
              "      <td>Sao Cha Không? Hay quá A Xìn ơi, A Quỳnh ơi</td>\n",
              "      <td>Tích cực</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>7</td>\n",
              "      <td>giọng anh Quuỳnh cất lên là bào cảm xúc dâng t...</td>\n",
              "      <td>Tích cực</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>8</td>\n",
              "      <td>Tôi xem đi xem lại trailer này để nghe Phan Mạ...</td>\n",
              "      <td>Tích cực</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>9</td>\n",
              "      <td>Phan Mạnh Quỳnh cất tiếng hát lên một cái thôi...</td>\n",
              "      <td>Tích cực</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   id                                            comment sentiment\n",
              "0   2  Bài của Phan Mạnh Quỳnh qá hay xuất sắc, hẹn b...  Tích cực\n",
              "1   3        Sao Cha Không? Hay quá A Xìn ơi, A Quỳnh ơi  Tích cực\n",
              "2   7  giọng anh Quuỳnh cất lên là bào cảm xúc dâng t...  Tích cực\n",
              "3   8  Tôi xem đi xem lại trailer này để nghe Phan Mạ...  Tích cực\n",
              "4   9  Phan Mạnh Quỳnh cất tiếng hát lên một cái thôi...  Tích cực"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NMx5TOeVkkii"
      },
      "source": [
        "# Implement help functions\n",
        "\n",
        "\n",
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c5razfnCENj1"
      },
      "source": [
        "## Normalized function for some specific cases in vietnamese"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IVkihQcrjEYy",
        "outputId": "2417ac37-e666-4715-cc68-39309d369af6"
      },
      "source": [
        "import re\n",
        "\n",
        "slang_dict = {\"bình thường\": [\"bth\",\"bt\",\"bthg\"], \"không\":[\"k\",\"K\",\"ko\",\"k0\",\"kO\",\"Ko\",\"khong\",\"kh\",\"khg\",\"hok\",\"hk\"], \n",
        "              \"rồi\":[\"r\", \"R\"], \"anh\":[\"a\",\"A\"], \"mình\":[\"mk\",\"Mk\"], \"vậy\":[\"z\",\"Z\",\"v\",\"V\"], \"gì\":[\"j\",\"J\",\"ji\"],\"trước\":[\"trc\"],\n",
        "              \"đi\":[\"ik\"], \"mọi người\": [\"mng\",\"Mng\",\"mn\",\"Mn\"],\"giáo viên\":[\"gv\"],\"thích\":[\"thix\"], \"được\":[\"đc\",\"Đc\",\"dc\"]}\n",
        "celeb_dict = {\"Trấn Thành\": [\"xìn\",\"Xìn\",\"TT\",\"tt\",\"Tran Thanh\",\"Tran thanh\"], \n",
        "              \"Phan Mạnh Quỳnh\": [\"pmq\", \"PMQ\", \"phan manh quynh\"]}\n",
        "en_dict = {\"trailer\": [\"traler\"], \"porn\":[\"pỏn\",\"Pỏn\"], \"free\":[\"fre\"], \"full\":[\"ful\"], \"facebook\":[\"facebok\"]}\n",
        "\n",
        "def create_norm_dict(input_dict):\n",
        "  norm = {}\n",
        "  for key, values in input_dict.items():\n",
        "    for value in values:\n",
        "      norm[value] = key\n",
        "  return norm\n",
        "\n",
        "def normalize(text, words):\n",
        "  regex = r\"\\b(?:\" + \"|\".join(re.escape(word) for word in words) + r\")\\b\"\n",
        "  reobj = re.compile(regex, re.I)\n",
        "  try:\n",
        "    res = reobj.sub(lambda x:words[x.group(0)], text)\n",
        "  except Exception as e:\n",
        "    res = text\n",
        "  return res\n",
        "\n",
        "bag_dicts = {}\n",
        "for d in (slang_dict,celeb_dict,en_dict): \n",
        "  bag_dicts.update(d)\n",
        "norm_dict = create_norm_dict(bag_dicts)\n",
        "print(norm_dict)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'bth': 'bình thường', 'bt': 'bình thường', 'bthg': 'bình thường', 'k': 'không', 'K': 'không', 'ko': 'không', 'k0': 'không', 'kO': 'không', 'Ko': 'không', 'khong': 'không', 'kh': 'không', 'khg': 'không', 'hok': 'không', 'hk': 'không', 'r': 'rồi', 'R': 'rồi', 'a': 'anh', 'A': 'anh', 'mk': 'mình', 'Mk': 'mình', 'z': 'vậy', 'Z': 'vậy', 'v': 'vậy', 'V': 'vậy', 'j': 'gì', 'J': 'gì', 'ji': 'gì', 'trc': 'trước', 'ik': 'đi', 'mng': 'mọi người', 'Mng': 'mọi người', 'mn': 'mọi người', 'Mn': 'mọi người', 'gv': 'giáo viên', 'thix': 'thích', 'đc': 'được', 'Đc': 'được', 'dc': 'được', 'xìn': 'Trấn Thành', 'Xìn': 'Trấn Thành', 'TT': 'Trấn Thành', 'tt': 'Trấn Thành', 'Tran Thanh': 'Trấn Thành', 'Tran thanh': 'Trấn Thành', 'pmq': 'Phan Mạnh Quỳnh', 'PMQ': 'Phan Mạnh Quỳnh', 'phan manh quynh': 'Phan Mạnh Quỳnh', 'traler': 'trailer', 'pỏn': 'porn', 'Pỏn': 'porn', 'fre': 'free', 'ful': 'full', 'facebok': 'facebook'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5QhfFQOck2bM"
      },
      "source": [
        "## Split and Demoji into normal words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SsONQXc4k_ZP"
      },
      "source": [
        "import emoji\n",
        "import functools\n",
        "import operator\n",
        "\n",
        "def split_demoji(text):\n",
        "  em_split_emoji = emoji.get_emoji_regexp().split(text)\n",
        "  em_split_whitespace = [substr.split() for substr in em_split_emoji]\n",
        "  em_split = functools.reduce(operator.concat, em_split_whitespace)\n",
        "  return emoji.demojize(' '. join(em_split))\n",
        "\n",
        "# test\n",
        "# split_demoji(df[\"comment\"][215])"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R1zo_KPesTR4"
      },
      "source": [
        "## Remove punctuations and special characters with Regex"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qi2UxNyksPqZ"
      },
      "source": [
        "def remove_punc(text):\n",
        "  text = text.replace('\\n', ' ')\n",
        "  text = re.sub(r\"[-()\\\"#/@;:<>{}`+=~|.!?,“”'‘’]\", ' ', text)\n",
        "  return text"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lnmY5dbAlAcb"
      },
      "source": [
        "# Preprocess with the following steps\n",
        "\n",
        "*   Step 1: Replace all links (http[s]://...) and timestamp (1:05, 2:08)\n",
        "*   Step 2: Normalize text for some specific cases (ko, k --> không, bth, bt --> bình thường, ...)\n",
        "*   Step 3: Split and demoji (❤️❤️❤️ --split into--> ❤️ ❤️ ❤️ --demoji--> :heart: :heart: :heart:)\n",
        "*   Step 4: Remove continuous duplicate words within text (hay quá điiiiiiiii)\n",
        "*   Step 5: Remove punctuations and special characters\n",
        "*   Step 6: Tokenize words\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fRSHu6Q0AZ4b"
      },
      "source": [
        "import itertools\n",
        "from underthesea import word_tokenize\n",
        "\n",
        "tokenize_list = []\n",
        "\n",
        "def preprocess(text):\n",
        "  # Replace all links and timestamp\n",
        "  text = re.sub('http[s]?://\\S+', '', text)\n",
        "  text = re.sub(r\"([\\d{1,2}:\\d{2}(:\\d{2})?])\", ' ', text)\n",
        "\n",
        "  # Normalize\n",
        "  text = normalize(text, norm_dict)\n",
        "\n",
        "  # Demojize\n",
        "  text = split_demoji(text)\n",
        "\n",
        "  # Process continuous duplicate words within text\n",
        "  text = ''.join(i for i, _ in itertools.groupby(text))\n",
        "\n",
        "  # Remove punctuations and special characters\n",
        "  text = remove_punc(text)\n",
        "\n",
        "  # word tokenize\n",
        "  tokens = word_tokenize(text)\n",
        "  tokenize_list.append(tokens)\n",
        "\n",
        "  return ' '.join(tokens).lower()\n",
        "\n",
        "# test preprocess\n",
        "# test_text = df[\"comment\"][210]\n",
        "# preprocess_text(test_text)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 258
        },
        "id": "RDSDyi6yZUpq",
        "outputId": "b4a6a2b7-ae4b-4a13-f882-05b3877e3c0e"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "processed_text = []\n",
        "for comment in df[\"comment\"]:\n",
        "  processed_text.append(preprocess(comment))\n",
        "  \n",
        "df['tokenize'] = np.array(tokenize_list)\n",
        "df['preprocessed'] = np.array(processed_text)\n",
        "df.head()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  import sys\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>comment</th>\n",
              "      <th>sentiment</th>\n",
              "      <th>tokenize</th>\n",
              "      <th>preprocessed</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2</td>\n",
              "      <td>Bài của Phan Mạnh Quỳnh qá hay xuất sắc, hẹn b...</td>\n",
              "      <td>Tích cực</td>\n",
              "      <td>[Bài, của, Phan Mạnh Quỳnh, qá, hay, xuất sắc,...</td>\n",
              "      <td>bài của phan mạnh quỳnh qá hay xuất sắc hẹn bố...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>3</td>\n",
              "      <td>Sao Cha Không? Hay quá A Xìn ơi, A Quỳnh ơi</td>\n",
              "      <td>Tích cực</td>\n",
              "      <td>[Sao, Cha, Không, Hay, quá, anh, Trấn Thành, ơ...</td>\n",
              "      <td>sao cha không hay quá anh trấn thành ơi anh qu...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>7</td>\n",
              "      <td>giọng anh Quuỳnh cất lên là bào cảm xúc dâng t...</td>\n",
              "      <td>Tích cực</td>\n",
              "      <td>[giọng, anh, Quỳnh, cất, lên, là, bào, cảm xúc...</td>\n",
              "      <td>giọng anh quỳnh cất lên là bào cảm xúc dâng tr...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>8</td>\n",
              "      <td>Tôi xem đi xem lại trailer này để nghe Phan Mạ...</td>\n",
              "      <td>Tích cực</td>\n",
              "      <td>[Tôi, xem, đi, xem, lại, trailer, này, để, ngh...</td>\n",
              "      <td>tôi xem đi xem lại trailer này để nghe phan mạ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>9</td>\n",
              "      <td>Phan Mạnh Quỳnh cất tiếng hát lên một cái thôi...</td>\n",
              "      <td>Tích cực</td>\n",
              "      <td>[Phan Mạnh Quỳnh, cất tiếng, hát, lên, một, cá...</td>\n",
              "      <td>phan mạnh quỳnh cất tiếng hát lên một cái thôi...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   id  ...                                       preprocessed\n",
              "0   2  ...  bài của phan mạnh quỳnh qá hay xuất sắc hẹn bố...\n",
              "1   3  ...  sao cha không hay quá anh trấn thành ơi anh qu...\n",
              "2   7  ...  giọng anh quỳnh cất lên là bào cảm xúc dâng tr...\n",
              "3   8  ...  tôi xem đi xem lại trailer này để nghe phan mạ...\n",
              "4   9  ...  phan mạnh quỳnh cất tiếng hát lên một cái thôi...\n",
              "\n",
              "[5 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EXufsJgQ6XK9"
      },
      "source": [
        "# Run Logistic Regression with Pipelines\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "93717fqRqdwG"
      },
      "source": [
        "import unidecode\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "\n",
        "class RemoveTone(BaseEstimator, TransformerMixin):\n",
        "    def remove_tone(self, s):\n",
        "        return unidecode.unidecode(s)\n",
        "\n",
        "    def transform(self, x):\n",
        "        return [self.remove_tone(s) for s in x]\n",
        "\n",
        "    def fit(self, x, y=None):\n",
        "        return self"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vWy9As7oUjDY",
        "outputId": "2c3f8f06-14c1-4344-f901-149fbbb446b8"
      },
      "source": [
        "from sklearn.pipeline import Pipeline, FeatureUnion\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import f1_score, accuracy_score\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(df[\"preprocessed\"], df[SENTIMENT_COLUMN_NAME], test_size=0.2, random_state=42)\n",
        "pipe = Pipeline(steps=[\n",
        "            ('features', FeatureUnion([\n",
        "                ('lower_pipe', Pipeline([\n",
        "                    ('tfidf', TfidfVectorizer(ngram_range=(1, 4), min_df=2))])),\n",
        "                ('with_tone_char', TfidfVectorizer(ngram_range=(1, 6), min_df=2, analyzer='char')),\n",
        "                ('remove_tone', Pipeline([\n",
        "                    ('remove_tone', RemoveTone()),\n",
        "                    ('tfidf', TfidfVectorizer(ngram_range=(1, 4), min_df=2))])),\n",
        "            ])),\n",
        "            ('estimator', LogisticRegression()),\n",
        "])\n",
        "\n",
        "pipe.fit(X_train, y_train)\n",
        "\n",
        "# Predict on train & test\n",
        "pred_train = pipe.predict(X_train)\n",
        "pred_test = pipe.predict(X_test)\n",
        "\n",
        "# Evaluate on train & test\n",
        "print(\"Accuracy on train\", accuracy_score(y_train, pred_train))\n",
        "print(\"Accuracy on test\", accuracy_score(y_test, pred_test))\n",
        "\n",
        "print(\"F1-score on train\", f1_score(y_train, pred_train, average=None))\n",
        "print(\"F1-score on test\", f1_score(y_test, pred_test, average=None))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy on train 0.9962859795728877\n",
            "Accuracy on test 0.7888888888888889\n",
            "F1-score on train [0.99579832 0.99617834 0.99664054]\n",
            "F1-score on test [0.73684211 0.76237624 0.83928571]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dmCmxrds6sMg"
      },
      "source": [
        "# Compare and Export results on Train & Test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iu7eSD15eamP"
      },
      "source": [
        "import os\n",
        "\n",
        "output_path = f\"{path}/output\"\n",
        "if not os.path.exists(output_path):\n",
        "    os.makedirs(output_path)\n",
        "\n",
        "df.to_excel(f\"{output_path}/logistic_processed.xlsx\", sheet_name='processed')\n",
        "print(f\"Please find the output files in path={output_path}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SsuHvDeeGIWx"
      },
      "source": [
        "def create_export_df(comment, sentiment, pred):\n",
        "  export_df = pd.DataFrame()\n",
        "  export_df[COMMENT_COLUMN_NAME]=np.array(comment)\n",
        "  export_df[SENTIMENT_COLUMN_NAME]=np.array(sentiment)\n",
        "  export_df['pred']=np.array(pred)\n",
        "  export_df[\"wrong_pred\"] = np.where(export_df[SENTIMENT_COLUMN_NAME] != export_df['pred'], True, False)\n",
        "  return export_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6WeJ31eKG4xr"
      },
      "source": [
        "train_df = create_export_df(X_train, y_train, pred_train)\n",
        "train_df.to_excel(f\"{output_path}/logistic_train.xlsx\", sheet_name='train')\n",
        "\n",
        "train_df[train_df[\"wrong_pred\"]==True]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7GL0kPeDepqm"
      },
      "source": [
        "test_df = create_export_df(X_test, y_test, pred_test)\n",
        "test_df.to_excel(f\"{output_path}/logistic_test.xlsx\", sheet_name='test')\n",
        "\n",
        "test_df[test_df[\"wrong_pred\"]==True]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QOWaaUP0Dl4g"
      },
      "source": [
        "# Issue notes\n",
        "- normalize dấu mũ\n",
        "- boost thêm những từ quan trọng\n",
        "- should remove celeb name? -> phim này khen nhưng phim khác chê -> tùy vào lượng cmt mà ra kết quả\n",
        "- xem phim xong trích câu trong truyện -> label tích cực?\n",
        "- một vài cmt tiếng anh ko đủ data để học"
      ]
    }
  ]
}